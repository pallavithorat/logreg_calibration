{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression & Calibration — Student Lab\n",
        "\n",
        "We focus on *probabilities*, not just accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Caliberation:  how accurate our predicted probabilities are as compared to actual outcomes\n",
        "# Model is well caliberated when for all the prediction withg probability p, p fraction of them belong to positive class\n",
        "# Imbalace data refers to the fact when i have classes which are not equally represented in the dataset\n",
        "# Ex: email is spam or not spam, imbalance data look like this : 95% not spam, 5% spam\n",
        "# Imbalance data cause many problems for ML models because they tend to be biased towards the majority class\n",
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic imbalanced data\n",
        "We simulate logits and labels with imbalance and miscalibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_rate 0.0712\n",
            "OK: in_0_1\n"
          ]
        }
      ],
      "source": [
        "# Base_rate = 0.05 means 5% of the data belongs to positive class\n",
        "# logit_scale controls the spread of the probabilities\n",
        "# miscalibration > 1 means model is overconfident, < 1 means underconfident\n",
        "def make_probs(n=5000, base_rate=0.05, logit_scale=1.0, miscalibration=1.0):\n",
        "    # Generate true probabilities via latent logits\n",
        "    z = logit_scale * rng.standard_normal(n)\n",
        "    # shift to get desired base rate approximately\n",
        "    z = z + np.log(base_rate/(1-base_rate))\n",
        "    p_true = 1/(1+np.exp(-z)) # true probabilities\n",
        "    y = (rng.random(n) < p_true).astype(int) # generates random numbers between 0 and 1,If it’s less than p_true, call it 1 else 0\n",
        "    z_model = miscalibration * z\n",
        "    p_model = 1/(1+np.exp(-z_model))\n",
        "    return y, p_model, p_true\n",
        "\n",
        "y, p_model, p_true = make_probs(miscalibration=2.0) \n",
        "print('base_rate', y.mean())\n",
        "check('in_0_1', np.all((p_model>=0) & (p_model<=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Metrics\n",
        "\n",
        "### Task 1.1: Confusion matrix metrics at a threshold\n",
        "Implement precision, recall, F1 at threshold t.\n",
        "\n",
        "# HINT:\n",
        "- y_hat = (p>=t)\n",
        "- TP/FP/FN\n",
        "\n",
        "**Checkpoint:** Why is accuracy misleading under imbalance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'tp': 2, 'fp': 2, 'fn': 354, 'precision': 0.499999999999875, 'recall': 0.005617977528089872, 'f1': 0.011111111111089075}\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix means at a given threshold t, how many true positives, true negatives, false positives, false negatives we have\n",
        "# We use it to evaluate classification models performance\n",
        "\n",
        "\n",
        "def metrics_at_threshold(y, p, t):\n",
        "    # TODO\n",
        "    y = y.astype(int)\n",
        "    yhat = (p >= t).astype(int)\n",
        "    tp = int(np.sum((y == 1) & (yhat == 1)) )# true positives\n",
        "    fp = int(np.sum((y == 0) & (yhat == 1)) )# false positives\n",
        "    fn = int(np.sum((y == 1) & (yhat == 0)) )# false negatives\n",
        "    prec = tp / (tp + fp + 1e-12 )  # precision = of all emails predicted as spam, how many were actually spam. High precison = few false alarms\n",
        "    rec = tp / (tp + fn + 1e-12 )   # recall = of all actual spam emails, how many were predicted as spam, High recall = miss very little spam\n",
        "    f1 = 2 * (prec * rec) / (prec + rec + 1e-12)  # F1 score\n",
        "    return {'tp': tp, 'fp': fp, 'fn': fn, 'precision': prec, 'recall': rec, 'f1': f1} \n",
        "   \n",
        "m = metrics_at_threshold(y, p_model, t=0.5) \n",
        "print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: PR curve area (approx)\n",
        "Compute a simple PR-AUC approximation by sorting thresholds.\n",
        "\n",
        "# HINT:\n",
        "- sort by p desc\n",
        "- compute precision/recall at each cut\n",
        "\n",
        "**Interview Angle:** when is PR-AUC preferable to ROC-AUC?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pr_auc 0.20170158178004188\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "# Reacall on x axis, Precision on y axis\n",
        "# PR curve area : single number summary of the precision-recall curve\n",
        "# it also measures the ability of the model to rank positive examples higher than negative examples\n",
        "# ROC curve area : it's used to evaluate the performance of binary classification models\n",
        "def pr_curve(y, p):\n",
        "    # TODO: return arrays (recall, precision)\n",
        "    order = np.argsort(-p)  # sort by predicted probabilities in descending order\n",
        "    y_sorted = y[order]\n",
        "    tp = np.cumsum(y_sorted == 1)  # true positives\n",
        "    fp = np.cumsum(y_sorted == 0) # false positives\n",
        "    prec = tp / (tp + fp + 1e-12) # precision means how many selected items are relevant\n",
        "    rec = tp / (tp[-1] + 1e-12) # recall means how many relevant items are selected\n",
        "    return rec, prec\n",
        "\n",
        "def auc_trapz(x, y):\n",
        "    # TODO\n",
        "    return float(np.trapz(y, x))  # area under curve using trapezoidal rule\n",
        "\n",
        "rec, prec = pr_curve(y, p_model)\n",
        "pr_auc = auc_trapz(rec, prec)\n",
        "print('pr_auc', pr_auc)\n",
        "check('finite', np.isfinite(pr_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Calibration\n",
        "\n",
        "### Task 2.1: Reliability curve + ECE\n",
        "\n",
        "Bin probabilities and compute:\n",
        "- avg predicted prob per bin\n",
        "- empirical accuracy per bin\n",
        "- ECE = sum (bin_weight * |acc - conf|)\n",
        "\n",
        "# HINT:\n",
        "- np.digitize\n",
        "\n",
        "**FAANG gotcha:** model can have good ranking but bad calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ECE 0.056267450617459955\n",
            "OK: ece_finite\n"
          ]
        }
      ],
      "source": [
        "# Caliberation : means how accurate our predicted probabilities are as compared to actual outcomes\n",
        "# the models predicted probabilities should match the actual outcomes\n",
        "# suppose model predicts 0.7(70%) probability , does that 70% really happen? \n",
        "# x-axis : predicted probability, y-axis : observed frequency\n",
        "# reliability diagram : plot of observed frequency vs predicted probability\n",
        "def reliability_bins(y, p, n_bins=10):\n",
        "    # TODO: return (bin_acc, bin_conf, bin_frac)\n",
        "    y = y.astype(int)\n",
        "    edges = np.linspace(0, 1, n_bins + 1)\n",
        "    b = np.digitize(p, edges[1:-1], right=False)  # bin indices\n",
        "    bin_acc = np.zeros(n_bins)  # accuracy in each bin\n",
        "    bin_conf = np.zeros(n_bins)  # average predicted prob in each bin\n",
        "    bin_frac = np.zeros(n_bins)  # fraction of samples in each bin\n",
        "    for i in range(n_bins):\n",
        "        mask = (b == i)\n",
        "        if mask.any():\n",
        "            bin_acc[i] = y[mask].mean()\n",
        "            bin_conf[i] = p[mask].mean()\n",
        "            bin_frac[i] = mask.mean()\n",
        "\n",
        "    return bin_acc, bin_conf, bin_frac\n",
        "\n",
        "\n",
        "def ece(bin_acc, bin_conf, bin_frac):\n",
        "    # TODO\n",
        "    return float(np.sum(bin_frac * np.abs(bin_acc - bin_conf)))  # expected calibration error\n",
        "\n",
        "bin_acc, bin_conf, bin_frac = reliability_bins(y, p_model, n_bins=10)\n",
        "ECE = ece(bin_acc, bin_conf, bin_frac)\n",
        "print('ECE', ECE)\n",
        "check('ece_finite', np.isfinite(ECE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Temperature scaling\n",
        "\n",
        "We assume p_model came from logits z_model. Approximate logits via logit(p).\n",
        "Then find temperature T that minimizes NLL on validation split: sigmoid(z/T).\n",
        "\n",
        "# HINT:\n",
        "- logit(p)=log(p/(1-p))\n",
        "- grid search T over [0.5..5]\n",
        "\n",
        "**Checkpoint:** Why does scaling logits preserve ranking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_T 2.0612244897959187\n",
            "ECE_before 0.051093775041383536 ECE_after 0.014975308183525216\n"
          ]
        }
      ],
      "source": [
        "# its a simple technique to improve calibration of a model by scaling the logits\n",
        "# temperature scaling : divide logits by a temperature parameter T\n",
        "# T > 1 means we are making the model less confident\n",
        "# T < 1 means we are making the model more confident\n",
        "def logit(p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def nll(y, p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))\n",
        "\n",
        "# TODO: split into val and fit T\n",
        "idx = rng.permutation(len(y))\n",
        "val = idx[: len(y)//2]\n",
        "test = idx[len(y)//2:]\n",
        "\n",
        "z = logit(p_model)\n",
        "\n",
        "ts = np.linspace(0.5, 5.0, 50)\n",
        "best_T = None\n",
        "best_loss = float('inf')\n",
        "for T in ts:\n",
        "    pT = 1/(1+np.exp(-(z[val]/T)))\n",
        "    loss = nll(y[val], pT)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_T = T\n",
        "pcal = 1/(1+np.exp(-z[test]/best_T))\n",
        "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
        "ECE_after = ece(*reliability_bins(y[test], pcal, 10))\n",
        "\n",
        "\n",
        "print('best_T', best_T)\n",
        "# apply temperature on test\n",
        "p_cal = 1/(1+np.exp(-(z[test]/best_T)))\n",
        "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
        "ECE_after = ece(*reliability_bins(y[test], p_cal, 10))\n",
        "print('ECE_before', ECE_before, 'ECE_after', ECE_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Thresholding with costs\n",
        "\n",
        "### Task 3.1: Pick threshold minimizing cost\n",
        "Cost = c_fp*FP + c_fn*FN\n",
        "\n",
        "# TODO: sweep thresholds and pick best.\n",
        "\n",
        "**Interview Angle:** map model probabilities to business decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t* 0.006 cost 2626.0\n"
          ]
        }
      ],
      "source": [
        "def best_threshold_cost(y, p, c_fp=1.0, c_fn=10.0):\n",
        "    # TODO\n",
        "    ts = np.linspace(0, 1, 501)\n",
        "    best_t = 0.5\n",
        "    best_cost = float('inf')\n",
        "    for t in ts:\n",
        "        y_hat = (p >= t).astype(int)\n",
        "        fp = np.sum((y == 0) & (y_hat == 1))\n",
        "        fn = np.sum((y == 1) & (y_hat == 0))\n",
        "        cost = c_fp * fp + c_fn * fn\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_t = t\n",
        "    return best_t, best_cost\n",
        "  \n",
        "\n",
        "t_star, cost_star = best_threshold_cost(y, p_model, c_fp=1.0, c_fn=10.0)\n",
        "print('t*', t_star, 'cost', cost_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- ECE computed\n",
        "- Temperature scaling applied\n",
        "- Threshold recommendation written\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
